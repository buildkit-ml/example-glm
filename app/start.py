import sys
import torch
import argparse

sys.path.append("./")
from glm_model import GLMModel
from SwissArmyTransformer.model import GLM130B
from SwissArmyTransformer import get_args
from SwissArmyTransformer.arguments import initialize_distributed


def add_bminf_args(parser):
    """Arguments for BMInf"""
    group = parser.add_argument_group("BMInf")
    group.add_argument("--bminf", action="store_true", help="Use BMInf to support low resource evaluation")
    group.add_argument("--bminf-memory-limit", type=int, default=20, help="Max memory for model per GPU (in GB)")
    return parser


def add_quantization_args(parser):
    group = parser.add_argument_group("Quantization")

    group.add_argument("--quantization-bit-width", type=int, default=None)
    group.add_argument("--from-quantized-checkpoint", action="store_true", help="Loading from a quantized checkpoint")


def add_generation_specific_args(parser):
    parser.add_argument("--sampling-strategy", type=str, default="BaseStrategy", help="Type of sampling strategy.")
    parser.add_argument("--min-gen-length", type=int, default=0, help="The minimum length each blank should generate.")
    parser.add_argument(
        "--print-all-beams", action="store_true", help="Print all output generated by beam search strategy."
    )


def foo_port_add_coordinator_args(parser):
    parser.add_argument('--batch-size', type=int, default=8, metavar='S',
                        help='batch-size for inference (default:8)')
    parser.add_argument("--model-id", type=str, default="someID", help="-")
    parser.add_argument("--upload-token", type=str, default="someToken, do not put it here", help="-")


def initialize(extra_args_provider):
    parser = argparse.ArgumentParser(add_help=False)
    add_bminf_args(parser)
    add_quantization_args(parser)
    GLM130B.add_model_specific_args(parser)
    extra_args_provider(parser)
    foo_port_add_coordinator_args(parser)
    known, args_list = parser.parse_known_args()
    print("initialize1: ", known)
    print("initialize2: ", args_list)
    initialize_distributed(args_list)
    args = get_args(args_list)
    args = argparse.Namespace(**vars(args), **vars(known))
    args.do_train = False
    print("initialize3: ", args)
    initialize_distributed(args)
    return args


if __name__ == "__main__":
    args = initialize(extra_args_provider=add_generation_specific_args)

    with torch.no_grad():
        fip = GLMModel(model_name='together.glm', args=args)
        fip.start()
