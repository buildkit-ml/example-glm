import sys
import argparse
sys.path.append("./")
from glm_model import GLMModel


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Local Inference Runner with coordinator.')
    parser.add_argument("--bminf", action="store_true", help="Use BMInf to support low resource evaluation")
    parser.add_argument("--bminf-memory-limit", type=int, default=20, help="Max memory for model per GPU (in GB)")
    parser.add_argument("--quantization-bit-width", type=int, default=None)
    parser.add_argument("--from-quantized-checkpoint", action="store_true", help="Loading from a quantized checkpoint")
    parser.add_argument("--sampling-strategy", type=str, default="BaseStrategy", help="Type of sampling strategy.")
    parser.add_argument("--min-gen-length", type=int, default=0, help="The minimum length each blank should generate.")
    parser.add_argument("--print-all-beams", action="store_true",
                        help="Print all output generated by beam search strategy.")
    parser.add_argument('--cuda-id', type=int, default=0, metavar='S',
                        help='cuda-id (default:0)')
    parser.add_argument('--batch-size', type=int, default=8, metavar='S',
                        help='batch-size for inference (default:8)')
    parser.add_argument('--fp16', action='store_true',
                        help='Run model in fp16 mode.')

    fip = GLMModel(model_name='together.glm', parser=parser)
    fip.start()
